---
title: "Lasso Regression"
author: "Justin Beresford"
format: 
  revealjs:
    theme: [default, style.scss]
    slide-number: true
    transition: fade
    incremental: true

---

## Bias vs Variance

In an ML context, we talk about a trade-off between bias and variance.

1. **What is Variance?**: Say you pick 100 poeple and estimate a $\hat{\beta}$, then pick another 100 people and estimate the same $\hat{\beta}$. Do that 100 times. How much do all those estimates vary?

2. **What is Bias**: is the mean of those 100 $\hat{\beta}$'s equal to the "true" $\beta$ in the full population. 

## Bias vs Variance

Economists are typically interested in unbiasedness, because they care about 'elasticities'. *"The effect of a unit increase in $X$ on $Y$ is $\beta$%"*.

::: {.fragment}
Machine Learning Engineers are typically interested in predicting things out-of-sample. They typically care more about low variance (i.e. stable estimates) than low bias.
:::

::: {.fragment}
Regularised regression is a way to [reduce variance]{.green}, but it comes at the cost of [increased bias]{.red}.

  => Lasso is one type of regularised regression
:::

::: {.section-slide .red-bg}
# Regularised regression
How does Lasso reduce variance?
:::

## Lasso Regression (intuition)

Occam's Razor: "Among competing models that explain the data equally well, prefer the simplest." 

If you have many (hundreds?) of predictors, use a model that only takes on a subset of them.

::: {.fragment}
In sample, adding more predictors always improves fit (RÂ² can only go up as you add $X$'s). But when predicting out-of-sample, those extra predictors might have made things worse (the "overfitting" problem).
:::

::: {.fragment}
Lasso regression asks: does adding this predictor improve the model [by enough to justify its inclusion?]{.red}
:::

## Recap on OLS

#### The OLS objective function

Standard OLS finds the $\beta$'s that minimise the sum of squared residuals:
$$\min_{\beta} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$

$$\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2$$

::: {.fragment}
Call this difference between $y_i$ and $\hat{y}_i$ the residual sum of squares $RSS$.

$$ RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$
:::


## Lasso 

#### The Lasso objective function

Lasso adds a penalty term to the OLS objective function:

$$\min_{\beta} RSS + \lambda \sum_{j=1}^{p} |\beta_j|$$

::: {.fragment}
This is saying: find the $\beta$'s that [(i)]{.red} minimise the RSS, but also [(ii)]{.red} keep the sum of the absolute values of the $\beta$'s small.
:::

::: {.fragment}

$\lambda$  is the penalty parameter. There's a trade-off between $RSS$ and keeping the $\beta$  small. How much do we care about one vs the other? 
NB: when $\lambda = 0$, this is just OLS. When it's $\infty$, all the $\beta$'s are zero.
:::

## Lasso for variable selection

Note we're minising the pure sum of $\beta$'s (as opposed to ${\beta}^2$ or some other non-linear function).

That means the loss function for the penalty term is linear: this has the effect of shrinking unhelpful coeffients all the way down to zero.

Lasso has a [variable selection]{.green} property: it will only include predictors that improve the model enough to justify their inclusion.

::: {.section-slide .green-bg}
# Pros and Cons of Lasso
:::

## Pros of Lasso

1. Improves out-of-sample prediction accuracy (by reducing variance and preventing overfitting).
1. Performs variable selection (i.e. sets some $\beta$'s to zero). Tells you which predictors are most important.
1. Great for high-dimensional data. Can even when you have more predictors than observations ($p > n$)
1. Can handle multi-collinearity (unlike OLS).

## Cons of Lasso

1. [Introduces bias]{.red}. You can't interpret the coefficients like you would in OLS. 
1. Very sensitive to the [choice of $\lambda$]{.red}.
1. If predictors are highly correlated, Lasso might arbitrarily select one and ignore the others
1. Unlike OLS, it doesn't provide confidence intervals or p-values. Although work-arounds exist. 

## Cons of Lasso

A [common fix]{.red} for these is to use Lasso for variable selection, then run an OLS regression with the variables that Lasso kept in. This way you get the p-values and can interpret coefficients as elasticities. 

::: {.section-slide .red-bg}
# Side note on other methods
Ridge and Elastic Net
:::

## Ridge Regression

Ridge regression is in the same family as Lasso, but uses a different penalty term:
$$\min_{\beta} RSS + \lambda \sum_{j=1}^{p} \beta_j^2$$

Because this minimises the sum of squared $\beta$'s, it penalises large $\beta$'s much more than small ones. Ridge also shrinks coefficients, but almost never to zero.

So while Lasso says ["pick a few strong predictors and drop the rest"]{.red}, Ridge says ["keep all predictors, but don't rely on any single one too heavily"]{.green}.

## Elastic Net

Elastic Net is a hybrid of Lasso and Ridge, combining both penalty terms:

$$
\min_{\beta} \; RSS 
+ \lambda \left( \alpha \sum_{j=1}^{p} |\beta_j|
+ (1 - \alpha) \sum_{j=1}^{p} \beta_j^2 \right)
$$

When $\alpha = 0$, this is pure Ridge. When $\alpha = 1$, it's pure Lasso. In between, it's a mix of both.

Drop the variables that don't add enough to justify their conclusion **and** of those that remain, don't rely too strongly on any single one. 